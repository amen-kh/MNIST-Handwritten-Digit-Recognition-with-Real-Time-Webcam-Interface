{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network for MNIST Classification\n",
    "\n",
    "The dataset is called MNIST and refers to handwritten digit recognition. You can find more about it on Yann LeCun's website (Director of AI Research, Facebook). He is one of the pioneers of what we've been talking about and of more complex approaches that are widely used today, such as covolutional neural networks (CNNs). \n",
    "\n",
    "The dataset provides 70,000 images (28x28 pixels) of handwritten digits (1 digit per image). \n",
    "\n",
    "we created an algorithm that detects which digit is written. Since there are only 10 digits (0, 1, 2, 3, 4, 5, 6, 7, 8, 9), this is a classification problem with 10 classes. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-datasets\n",
      "  Downloading tensorflow_datasets-4.9.7-py3-none-any.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: absl-py in c:\\users\\home\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-datasets) (2.1.0)\n",
      "Collecting click (from tensorflow-datasets)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting dm-tree (from tensorflow-datasets)\n",
      "  Downloading dm_tree-0.1.8-cp312-cp312-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting immutabledict (from tensorflow-datasets)\n",
      "  Downloading immutabledict-4.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\home\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-datasets) (2.0.2)\n",
      "Collecting promise (from tensorflow-datasets)\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: protobuf>=3.20 in c:\\users\\home\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-datasets) (5.28.3)\n",
      "Requirement already satisfied: psutil in c:\\users\\home\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-datasets) (6.0.0)\n",
      "Collecting pyarrow (from tensorflow-datasets)\n",
      "  Downloading pyarrow-18.0.0-cp312-cp312-win_amd64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\home\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-datasets) (2.32.3)\n",
      "Collecting simple-parsing (from tensorflow-datasets)\n",
      "  Downloading simple_parsing-0.1.6-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting tensorflow-metadata (from tensorflow-datasets)\n",
      "  Downloading tensorflow_metadata-1.16.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: termcolor in c:\\users\\home\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-datasets) (2.5.0)\n",
      "Collecting toml (from tensorflow-datasets)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting tqdm (from tensorflow-datasets)\n",
      "  Downloading tqdm-4.66.6-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: wrapt in c:\\users\\home\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-datasets) (1.16.0)\n",
      "Collecting etils>=1.9.1 (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets)\n",
      "  Downloading etils-1.10.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting fsspec (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets)\n",
      "  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting importlib_resources (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets)\n",
      "  Downloading importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\home\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (4.12.2)\n",
      "Collecting zipp (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets)\n",
      "  Downloading zipp-3.20.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\home\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\home\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\home\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\home\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\home\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->tensorflow-datasets) (0.4.6)\n",
      "Requirement already satisfied: six in c:\\users\\home\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from promise->tensorflow-datasets) (1.16.0)\n",
      "Collecting docstring-parser<1.0,>=0.15 (from simple-parsing->tensorflow-datasets)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting googleapis-common-protos<2,>=1.56.4 (from tensorflow-metadata->tensorflow-datasets)\n",
      "  Downloading googleapis_common_protos-1.65.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading tensorflow_datasets-4.9.7-py3-none-any.whl (5.3 MB)\n",
      "   ---------------------------------------- 0.0/5.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/5.3 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.8/5.3 MB 2.2 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 1.3/5.3 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 1.8/5.3 MB 2.3 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 2.4/5.3 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 2.6/5.3 MB 2.3 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 3.1/5.3 MB 2.3 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 3.7/5.3 MB 2.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 4.2/5.3 MB 2.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 4.7/5.3 MB 2.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.0/5.3 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.3/5.3 MB 2.3 MB/s eta 0:00:00\n",
      "Downloading etils-1.10.0-py3-none-any.whl (164 kB)\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading dm_tree-0.1.8-cp312-cp312-win_amd64.whl (101 kB)\n",
      "Downloading immutabledict-4.2.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading pyarrow-18.0.0-cp312-cp312-win_amd64.whl (25.1 MB)\n",
      "   ---------------------------------------- 0.0/25.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/25.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.8/25.1 MB 2.2 MB/s eta 0:00:11\n",
      "   -- ------------------------------------- 1.3/25.1 MB 2.3 MB/s eta 0:00:11\n",
      "   -- ------------------------------------- 1.8/25.1 MB 2.3 MB/s eta 0:00:11\n",
      "   --- ------------------------------------ 2.4/25.1 MB 2.3 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 2.6/25.1 MB 2.3 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 3.1/25.1 MB 2.3 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 3.7/25.1 MB 2.3 MB/s eta 0:00:10\n",
      "   ------ --------------------------------- 4.2/25.1 MB 2.3 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 4.7/25.1 MB 2.3 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 5.2/25.1 MB 2.3 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 5.5/25.1 MB 2.3 MB/s eta 0:00:09\n",
      "   --------- ------------------------------ 6.0/25.1 MB 2.3 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 6.6/25.1 MB 2.3 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.1/25.1 MB 2.3 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 7.6/25.1 MB 2.3 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 8.1/25.1 MB 2.3 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 8.7/25.1 MB 2.3 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 8.9/25.1 MB 2.3 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 9.4/25.1 MB 2.3 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 10.0/25.1 MB 2.3 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 10.5/25.1 MB 2.3 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 11.0/25.1 MB 2.3 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 11.3/25.1 MB 2.3 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 11.8/25.1 MB 2.3 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 12.3/25.1 MB 2.3 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 12.8/25.1 MB 2.3 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.4/25.1 MB 2.3 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.6/25.1 MB 2.3 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 14.2/25.1 MB 2.3 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 14.7/25.1 MB 2.3 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.2/25.1 MB 2.3 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.7/25.1 MB 2.3 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 16.3/25.1 MB 2.3 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 16.5/25.1 MB 2.3 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 17.0/25.1 MB 2.3 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.6/25.1 MB 2.3 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 18.1/25.1 MB 2.3 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.6/25.1 MB 2.3 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 19.1/25.1 MB 2.3 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.7/25.1 MB 2.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.2/25.1 MB 2.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.4/25.1 MB 2.3 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 21.0/25.1 MB 2.3 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.5/25.1 MB 2.3 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 22.0/25.1 MB 2.3 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 22.5/25.1 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.8/25.1 MB 2.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.3/25.1 MB 2.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.9/25.1 MB 2.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.4/25.1 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.9/25.1 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.1/25.1 MB 2.3 MB/s eta 0:00:00\n",
      "Downloading simple_parsing-0.1.6-py3-none-any.whl (112 kB)\n",
      "Downloading tensorflow_metadata-1.16.1-py3-none-any.whl (28 kB)\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading tqdm-4.66.6-py3-none-any.whl (78 kB)\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading googleapis_common_protos-1.65.0-py2.py3-none-any.whl (220 kB)\n",
      "Downloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Downloading importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
      "Downloading zipp-3.20.2-py3-none-any.whl (9.2 kB)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21544 sha256=e36b3a17e4344e686d4863815a010a4ed294435dfcd91494fa73c664e6af0fbd\n",
      "  Stored in directory: c:\\users\\home\\appdata\\local\\pip\\cache\\wheels\\e7\\e6\\28\\864bdfee5339dbd6ddcb5a186286a8e217648ec198bdf0097d\n",
      "Successfully built promise\n",
      "Installing collected packages: dm-tree, zipp, tqdm, toml, pyarrow, promise, importlib_resources, immutabledict, googleapis-common-protos, fsspec, etils, docstring-parser, click, tensorflow-metadata, simple-parsing, tensorflow-datasets\n",
      "Successfully installed click-8.1.7 dm-tree-0.1.8 docstring-parser-0.16 etils-1.10.0 fsspec-2024.10.0 googleapis-common-protos-1.65.0 immutabledict-4.2.0 importlib_resources-6.4.5 promise-2.3 pyarrow-18.0.0 simple-parsing-0.1.6 tensorflow-datasets-4.9.7 tensorflow-metadata-1.16.1 toml-0.10.2 tqdm-4.66.6 zipp-3.20.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow-datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# TensorFLow includes a data provider for MNIST that we'll use.\n",
    "import tensorflow_datasets as tfds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Load MNIST dataset\n",
    "\n",
    "That's where we load and preprocess our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train, mnist_test = mnist_dataset['train'],mnist_dataset['test']\n",
    "\n",
    "# Calculate validation and test samples\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "num_validation_samples = tf.cast(num_validation_samples , tf.int64 )\n",
    "num_test_samples = mnist_info.splits['test'].num_examples \n",
    "num_test_samples = tf.cast(num_test_samples , tf.int64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define scaling and reshaping function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_and_reshape(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = image / 255.0\n",
    "    image = tf.reshape(image, (28, 28, 1))  # Ensure image shape is (28, 28, 1)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "scaled_train_and_validation_data = mnist_train.map(scale_and_reshape) \n",
    "test_data = mnist_test.map(scale_and_reshape) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle and batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we should shuffle the data for efficient SGD \n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "#we are using batching so we have to set the batch size \n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "train_data = train_data.batch(batch_size) \n",
    "validation_data = validation_data.batch(num_validation_samples) \n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "validation_inputs , validation_targets = next(iter(validation_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### outline the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#784 input layers , 50 hidden layer , 50 , 10 outputs \n",
    "#28*28=784\n",
    "input_size=784\n",
    "#digits(0-9)\n",
    "output_size = 10 \n",
    "hidden_layer_size = 1000 #the width \n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    #we flatten any multidimmensional arrays like picture because the dense layers operates only on vectors(1d array)\n",
    "    #each image has a shape of (28,28,1) after the flatten its shape become (784,)\n",
    "    tf.keras.Input(shape=(28, 28, 1)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    #the dense layer is here to calculate weights and biases\n",
    "    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)\n",
    "    tf.keras.layers.Dense(hidden_layer_size , activation='relu'),#1st hidden layer\n",
    "    tf.keras.layers.Dropout(0.3),  # Dropout after first hidden layer, prevent from overfiting \n",
    "    \n",
    "    tf.keras.layers.Dense(hidden_layer_size , activation='relu'),#2nd hidden layer \n",
    "    tf.keras.layers.Dropout(0.3),  # Dropout after second hidden layer\n",
    "    \n",
    "    tf.keras.layers.Dense(hidden_layer_size , activation='relu'),#3rd hidden layer \n",
    "    tf.keras.layers.Dropout(0.3),  # Dropout after third hidden layer\n",
    "    \n",
    "    # we use softmax in the output because we need probabilities as outputs (suitable for multi-class classification)\n",
    "    tf.keras.layers.Dense(output_size , activation='softmax')\n",
    "    \n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### choose the optimizer and the loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_mnist_model.keras', save_best_only=True, save_weights_only=False, monitor='val_loss', mode='min')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "540/540 - 10s - 19ms/step - accuracy: 0.9223 - loss: 0.2526 - val_accuracy: 0.9683 - val_loss: 0.1123 - learning_rate: 1.0000e-03\n",
      "Epoch 2/10\n",
      "540/540 - 8s - 16ms/step - accuracy: 0.9633 - loss: 0.1220 - val_accuracy: 0.9723 - val_loss: 0.0860 - learning_rate: 1.0000e-03\n",
      "Epoch 3/10\n",
      "540/540 - 8s - 16ms/step - accuracy: 0.9713 - loss: 0.0935 - val_accuracy: 0.9827 - val_loss: 0.0607 - learning_rate: 1.0000e-03\n",
      "Epoch 4/10\n",
      "540/540 - 8s - 16ms/step - accuracy: 0.9761 - loss: 0.0798 - val_accuracy: 0.9833 - val_loss: 0.0590 - learning_rate: 1.0000e-03\n",
      "Epoch 5/10\n",
      "540/540 - 8s - 15ms/step - accuracy: 0.9784 - loss: 0.0679 - val_accuracy: 0.9838 - val_loss: 0.0552 - learning_rate: 1.0000e-03\n",
      "Epoch 6/10\n",
      "540/540 - 8s - 15ms/step - accuracy: 0.9799 - loss: 0.0652 - val_accuracy: 0.9880 - val_loss: 0.0422 - learning_rate: 1.0000e-03\n",
      "Epoch 7/10\n",
      "540/540 - 8s - 15ms/step - accuracy: 0.9825 - loss: 0.0593 - val_accuracy: 0.9845 - val_loss: 0.0570 - learning_rate: 1.0000e-03\n",
      "Epoch 8/10\n",
      "540/540 - 8s - 16ms/step - accuracy: 0.9839 - loss: 0.0540 - val_accuracy: 0.9915 - val_loss: 0.0341 - learning_rate: 1.0000e-03\n",
      "Epoch 9/10\n",
      "540/540 - 8s - 15ms/step - accuracy: 0.9853 - loss: 0.0504 - val_accuracy: 0.9888 - val_loss: 0.0425 - learning_rate: 1.0000e-03\n",
      "Epoch 10/10\n",
      "540/540 - 8s - 15ms/step - accuracy: 0.9864 - loss: 0.0456 - val_accuracy: 0.9908 - val_loss: 0.0426 - learning_rate: 1.0000e-03\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x137f82ad070>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we used early stopping technique \n",
    "\n",
    "num_epochs = 10\n",
    "model.fit(train_data,\n",
    "          epochs=num_epochs,\n",
    "          validation_data=(validation_inputs, validation_targets),\n",
    "          callbacks=[early_stopping, model_checkpoint, reduce_lr],\n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 383ms/step - accuracy: 0.9810 - loss: 0.0728\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.07. Test accuracy: 98.10%\n"
     ]
    }
   ],
   "source": [
    "# We can apply some nice formatting if we want to\n",
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('final_mnist_model.keras')\n",
    "print(\"Model saved as 'final_mnist_model.keras'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
